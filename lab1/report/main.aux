\relax 
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}CLASSIFICATION WITH ONE LAYER PERCEPTRON}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}Generating the training data}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}Implementation of the Delta rule}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {III}CLASSIFICATION WITH THE TWO LAYER PERCEPTRON}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:s}{{1a}{2}}
\newlabel{sub@fig:s}{{a}{2}}
\newlabel{fig:ns}{{1b}{2}}
\newlabel{sub@fig:ns}{{b}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Decision boundary estimated using a single layer perceptron for different sets of data. In particular we have binary-labeled data (+1, -1).\relax }}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}The forward pass}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}The backward pass and weight update}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-C}All together}{2}}
\newlabel{fig:s}{{2a}{3}}
\newlabel{sub@fig:s}{{a}{3}}
\newlabel{fig:ns}{{2b}{3}}
\newlabel{sub@fig:ns}{{b}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces MSE curve for two different cases.\relax }}{3}}
\newlabel{f:2}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-D}The encoder problem}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Mean square error of the encoder.\relax }}{3}}
\newlabel{f:enc}{{3}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Results of the hidden layer\relax }}{3}}
\newlabel{t:1}{{I}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}FUNCTION APPROXIMATION}{3}}
\newlabel{fig:31}{{4a}{4}}
\newlabel{sub@fig:31}{{a}{4}}
\newlabel{fig:32}{{4b}{4}}
\newlabel{sub@fig:32}{{b}{4}}
\newlabel{fig:33}{{4c}{4}}
\newlabel{sub@fig:33}{{c}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Neural network with one hidden layer approximates the given function $f(x,y)$ for different number of neurons in the hidden layer.\relax }}{4}}
\newlabel{f:3}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Mean square error of the encoder.\relax }}{5}}
\newlabel{f:4}{{5}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {V}GENERALIZATION}{5}}
\newlabel{fig:41}{{6a}{5}}
\newlabel{sub@fig:41}{{a}{5}}
\newlabel{fig:42}{{6b}{5}}
\newlabel{sub@fig:42}{{b}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Estimation using different number of training samples. The whole set of training samples was of 125.\relax }}{5}}
\newlabel{f:5}{{6}{5}}
